---
title: "p8105_hw6_yd2739"
author: "Yuxuan Du"
date: "2023-11-17"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(ggplot2)
library(modelr)
library(mgcv)
```

## Problem 1

```{r}
homicides_data = read_csv('https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv')|>
  mutate(city_state = paste(city, state, sep = ", "))|>
  group_by(city_state)|>
  subset(city_state != "Dallas, TX")|>
  subset(city_state != "Phoenix, AZ")|>
  subset(city_state != "Kansas City, MO")
```

```{r}
homicides_data_cleaned = homicides_data|>
  mutate(case_solved = ifelse(disposition!="Open/No arrest", 1, 0))|>
  filter(victim_race == "White"|victim_race == "Black")|>
  mutate(victim_age = as.numeric(victim_age))
```

```{r}
Baltimore_df = homicides_data_cleaned|>
  subset(city_state == "Baltimore, MD")|>
  mutate(victim_race = factor(victim_race))|>
  mutate(victim_sex = factor(victim_sex))

fit_logistic = 
  Baltimore_df|> 
  glm(case_solved ~ victim_age + victim_race + victim_sex, data = _, family = binomial())

fit_logistic|>
  broom::tidy()|> 
  filter(term == "victim_sexMale")|>
  mutate(
    OR = exp(estimate),
    LowerCI = exp(estimate - 1.96 * std.error),
    UpperCI = exp(estimate + 1.96 * std.error)
  )
```


```{r}
fit_glm_for_city = function(city_data) {
  fit_logistic = glm(case_solved ~ victim_age + victim_race + victim_sex, data = city_data, family = binomial())
  tidy_result = fit_logistic|>
    broom::tidy()|>
    filter(term == "victim_sexMale")|>
    mutate(
      OR = exp(estimate),
      LowerCI = exp(estimate - 1.96 * std.error),
      UpperCI = exp(estimate + 1.96 * std.error)
    )|>
    select(term, OR, LowerCI, UpperCI)
}

result_list = homicides_data_cleaned |>
  mutate(victim_race = factor(victim_race))|>
  mutate(victim_sex = factor(victim_sex))|>
  group_by(city_state) |>
  nest()|>
  filter(city_state != "Tulsa, AL")|>
  mutate(glm_result = map(data, fit_glm_for_city))|>
  unnest(glm_result)
```

```{r}
result_list|>
  ggplot(aes(x = reorder(city_state, OR), y = OR))+
  geom_point() +
  geom_errorbar(aes(ymin = LowerCI, ymax = UpperCI), width = 0.2) +
  coord_flip() +  # Flips the axes to make the cities display horizontally
  labs(title = "Estimated ORs and CIs for Solving Homicides by City",
       x = "City",
       y = "Adjusted Odds Ratio (OR)")
```

## Problem 2

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

```{r}
boot_straps_result = 
  weather_df |> 
  modelr::bootstrap(n = 5000)|>
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin + prcp, data = df)),
    results = map(models, broom::tidy), 
    glance = map(models, broom::glance))|>
  select(-strap, -models)
  

betas_result = boot_straps_result|>
  unnest(results)|>
  group_by(.id)|>
  filter(term != "(Intercept)")|>
  summarize(coefficient_product = prod(estimate))|>
  mutate(log_coefficient_product = log(coefficient_product))
    
r_squared_result = boot_straps_result|>
  unnest(glance)|>
  subset(select = c(.id, r.squared))
```

```{r}
p2_result = left_join(betas_result, r_squared_result, by = ".id")
```

```{r}
p2_result|>
  ggplot(aes(x = log_coefficient_product))+
  geom_density()
```

```{r}
p2_result|>
  pull(coefficient_product)|>
  quantile(c(0.025, 0.975), na.rm = TRUE)
```
We could see that the coefficient product's log result is left skewed. There are also lots of Na in the dataset caused by negative product of the coefficient. The 2.5% quantile of coefficient product's log is -0.009531940, the 97.5% quantile of coefficient product's log is 0.007972366. 
```{r}
p2_result|>
  ggplot(aes(x = r.squared))+
  geom_density()
```

```{r}
p2_result|>
  pull(r.squared)|>
  quantile(c(0.025, 0.975), na.rm = TRUE)
```
The r^2 is less skewed and is approximately normal. We could see it is still slightly left skewed. The 2.5% quantile of coefficient product's log is 0.8889543, the 97.5% quantile of coefficient product's log is 0.9407838. 

## Problem 3
```{r}
#male  = 1, female = 0
birthweight_df = read_csv("data/birthweight.csv")|>
  mutate(babysex = ifelse(babysex == 1, 1, 0))|>
  mutate(
    babysex = factor(babysex),
    frace = factor(frace),
    mrace = factor(mrace),
    frace = fct_infreq(frace),
    mrace = fct_infreq(mrace))
```

So, first considerndata leakage, I will not include any data that gathered after baby birth. So babysex, bhead, blength are not included in our model. Also, ppbmi, ppwt, mheigth, wtgain might have multicollineality problem. I decide drop ppbmi and wtgain.(since these are all calculated by other variables). I'm not sure malform is detected after or before giving birth, so I also excluded it. 

Then I choose to fit a model and decide variables I will keep using their coefficient's p-value.

```{r}
tes_lm = lm(bwt ~  delwt + fincome + frace + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppwt +  menarche + gaweeks + smoken, data = birthweight_df)
summary(tes_lm)
```

Obviously there are colinearlity in our dataset and caused Na coeffcient. We will exclude pnumsga, pnumlbw in our analysis. Also, we only choose the most significant variables. They are : delwt, mheight, ppwt, gaweeks, smoken. 

I think my model will contain motherâ€™s weight at delivery (pounds), gestational age in weeks, smoken, 

```{r}
proposed_lm = lm(bwt ~  delwt + mheight + ppwt + gaweeks + smoken, data = birthweight_df)
prediction_result = birthweight_df|>
  subset(select = c(delwt, mheight, ppwt, gaweeks, smoken, bwt))|>
  modelr::add_predictions(proposed_lm)|>
  modelr::add_residuals(proposed_lm)
```

```{r}
cv_df = 
  crossv_mc(birthweight_df, 100)
  
cv_df =
  cv_df |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```

```{r}
cv_df = 
  cv_df |> 
  mutate(
    proposed_mod  = map(train, \(df) lm(bwt ~ delwt + mheight + ppwt + gaweeks + smoken, data = df)),
    length_gestation_mod  = map(train, \(df) lm(bwt ~  blength + gaweeks, data = df)),
    interact_mod  = map(train, \(df) lm(bwt ~  bhead + blength + babysex + bhead * blength + blength* babysex + bhead* babysex + bhead * blength * babysex, data = df))) |> 
  mutate(
    rmse_proposed = map2_dbl(proposed_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_length_gestation = map2_dbl(length_gestation_mod, test, \(mod, df) rmse(model = mod, data = df)),
    rmse_interact = map2_dbl(interact_mod, test, \(mod, df) rmse(model = mod, data = df)))
```

```{r}
cv_df |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

We could see that the model with interaction have the lowest RMSE, which means it is the model with best predictivity. 